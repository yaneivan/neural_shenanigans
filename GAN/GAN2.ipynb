{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3326d580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73c9e680",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "n_epochs = 1000\n",
    "batch_size = 64\n",
    "lr = 0.0002\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "n_cpu = 8\n",
    "latent_dim = 100\n",
    "img_size = 128\n",
    "channels = 3\n",
    "sample_interval = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b65e7de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddfb1b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.init_size = img_size // 4\n",
    "        self.l1 = nn.Sequential(nn.Linear(latent_dim, 128 * self.init_size ** 2))\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, z):\n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d641657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(channels, 16, bn=False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128),\n",
    "        )\n",
    "\n",
    "        # The height and width of downsampled image\n",
    "        ds_size = img_size // 2 ** 4\n",
    "        self.adv_layer = nn.Sequential(nn.Linear(512*16, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.model(img)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.adv_layer(out)\n",
    "\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "329235a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Dropout2d(p=0.25, inplace=False)\n",
       "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Dropout2d(p=0.25, inplace=False)\n",
       "    (6): BatchNorm2d(32, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (9): Dropout2d(p=0.25, inplace=False)\n",
       "    (10): BatchNorm2d(64, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (12): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (13): Dropout2d(p=0.25, inplace=False)\n",
       "    (14): BatchNorm2d(128, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (adv_layer): Sequential(\n",
       "    (0): Linear(in_features=8192, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss function\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "adversarial_loss.to(device)\n",
    "\n",
    "# Initialize weights\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f5b677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure data loader\n",
    "os.makedirs(\"/data/mnist\", exist_ok=True)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        \"/data/mnist\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.Resize(img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a856ee62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(root=\"cats\", \n",
    "                          transform=transforms.Compose([\n",
    "                              transforms.Resize(img_size),\n",
    "                              transforms.CenterCrop(img_size), \n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                          ]))\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size = batch_size,\n",
    "                                        shuffle=True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd14b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 0/1000] [Batch 246/247] [D loss: 0.748594] [G loss: 0.653998]\n",
      "[Epoch 1/1000] [Batch 246/247] [D loss: 0.693725] [G loss: 0.712708]\n",
      "[Epoch 2/1000] [Batch 246/247] [D loss: 0.688455] [G loss: 0.705525]\n",
      "[Epoch 3/1000] [Batch 246/247] [D loss: 0.665830] [G loss: 0.711854]\n",
      "[Epoch 4/1000] [Batch 246/247] [D loss: 0.705281] [G loss: 0.732448]\n",
      "[Epoch 5/1000] [Batch 246/247] [D loss: 0.678582] [G loss: 0.696422]\n",
      "[Epoch 6/1000] [Batch 246/247] [D loss: 0.684510] [G loss: 0.744285]\n",
      "[Epoch 7/1000] [Batch 246/247] [D loss: 0.655462] [G loss: 0.741263]\n",
      "[Epoch 8/1000] [Batch 246/247] [D loss: 0.653474] [G loss: 0.702507]\n",
      "[Epoch 9/1000] [Batch 246/247] [D loss: 0.680369] [G loss: 0.662454]\n",
      "[Epoch 10/1000] [Batch 246/247] [D loss: 0.724844] [G loss: 0.722923]\n",
      "[Epoch 11/1000] [Batch 246/247] [D loss: 0.703375] [G loss: 0.697721]\n",
      "[Epoch 12/1000] [Batch 246/247] [D loss: 0.704029] [G loss: 0.709544]\n",
      "[Epoch 13/1000] [Batch 246/247] [D loss: 0.700805] [G loss: 0.713903]\n",
      "[Epoch 14/1000] [Batch 246/247] [D loss: 0.683576] [G loss: 0.706956]\n",
      "[Epoch 15/1000] [Batch 246/247] [D loss: 0.708519] [G loss: 0.697086]\n",
      "[Epoch 16/1000] [Batch 246/247] [D loss: 0.704582] [G loss: 0.687826]\n",
      "[Epoch 17/1000] [Batch 246/247] [D loss: 0.698907] [G loss: 0.705621]\n",
      "[Epoch 18/1000] [Batch 246/247] [D loss: 0.697151] [G loss: 0.690288]\n",
      "[Epoch 19/1000] [Batch 246/247] [D loss: 0.692125] [G loss: 0.687829]\n",
      "[Epoch 20/1000] [Batch 246/247] [D loss: 0.690740] [G loss: 0.694965]\n",
      "[Epoch 21/1000] [Batch 246/247] [D loss: 0.695895] [G loss: 0.694671]\n",
      "[Epoch 22/1000] [Batch 246/247] [D loss: 0.696211] [G loss: 0.695105]\n",
      "[Epoch 23/1000] [Batch 246/247] [D loss: 0.696532] [G loss: 0.704067]\n",
      "[Epoch 24/1000] [Batch 246/247] [D loss: 0.695249] [G loss: 0.683554]\n",
      "[Epoch 25/1000] [Batch 246/247] [D loss: 0.697516] [G loss: 0.687105]\n",
      "[Epoch 26/1000] [Batch 246/247] [D loss: 0.693117] [G loss: 0.692921]\n",
      "[Epoch 27/1000] [Batch 246/247] [D loss: 0.690257] [G loss: 0.693592]\n",
      "[Epoch 28/1000] [Batch 246/247] [D loss: 0.693809] [G loss: 0.694881]\n",
      "[Epoch 29/1000] [Batch 246/247] [D loss: 0.694300] [G loss: 0.689391]\n",
      "[Epoch 30/1000] [Batch 246/247] [D loss: 0.692800] [G loss: 0.692590]\n",
      "[Epoch 31/1000] [Batch 246/247] [D loss: 0.694086] [G loss: 0.695836]\n",
      "[Epoch 32/1000] [Batch 246/247] [D loss: 0.689770] [G loss: 0.689083]\n",
      "[Epoch 33/1000] [Batch 246/247] [D loss: 0.694163] [G loss: 0.693514]\n",
      "[Epoch 34/1000] [Batch 246/247] [D loss: 0.691947] [G loss: 0.686153]\n",
      "[Epoch 35/1000] [Batch 246/247] [D loss: 0.699615] [G loss: 0.705578]\n",
      "[Epoch 36/1000] [Batch 246/247] [D loss: 0.693357] [G loss: 0.697891]\n",
      "[Epoch 37/1000] [Batch 246/247] [D loss: 0.689651] [G loss: 0.688735]\n",
      "[Epoch 38/1000] [Batch 246/247] [D loss: 0.693941] [G loss: 0.697398]\n",
      "[Epoch 39/1000] [Batch 246/247] [D loss: 0.690250] [G loss: 0.695151]\n",
      "[Epoch 40/1000] [Batch 246/247] [D loss: 0.695137] [G loss: 0.710593]\n",
      "[Epoch 41/1000] [Batch 246/247] [D loss: 0.701210] [G loss: 0.691521]\n",
      "[Epoch 42/1000] [Batch 246/247] [D loss: 0.698014] [G loss: 0.682041]\n",
      "[Epoch 43/1000] [Batch 246/247] [D loss: 0.649424] [G loss: 0.614179]\n",
      "[Epoch 44/1000] [Batch 246/247] [D loss: 0.690470] [G loss: 0.700132]\n",
      "[Epoch 45/1000] [Batch 246/247] [D loss: 0.695972] [G loss: 0.683065]\n",
      "[Epoch 46/1000] [Batch 246/247] [D loss: 0.689639] [G loss: 0.681307]\n",
      "[Epoch 47/1000] [Batch 246/247] [D loss: 0.698225] [G loss: 0.690161]\n",
      "[Epoch 48/1000] [Batch 246/247] [D loss: 0.694085] [G loss: 0.682143]\n",
      "[Epoch 49/1000] [Batch 246/247] [D loss: 0.716993] [G loss: 0.693974]\n",
      "[Epoch 50/1000] [Batch 246/247] [D loss: 0.701154] [G loss: 0.681149]\n",
      "[Epoch 51/1000] [Batch 246/247] [D loss: 0.692484] [G loss: 0.687207]\n",
      "[Epoch 52/1000] [Batch 246/247] [D loss: 0.715850] [G loss: 0.638031]\n",
      "[Epoch 53/1000] [Batch 246/247] [D loss: 0.699479] [G loss: 0.713786]\n",
      "[Epoch 54/1000] [Batch 246/247] [D loss: 0.678961] [G loss: 0.716303]\n",
      "[Epoch 55/1000] [Batch 246/247] [D loss: 0.689884] [G loss: 0.700234]\n",
      "[Epoch 56/1000] [Batch 246/247] [D loss: 0.690936] [G loss: 0.680539]\n",
      "[Epoch 57/1000] [Batch 246/247] [D loss: 0.696149] [G loss: 0.687807]\n",
      "[Epoch 58/1000] [Batch 246/247] [D loss: 0.710519] [G loss: 0.692356]\n",
      "[Epoch 59/1000] [Batch 246/247] [D loss: 0.687483] [G loss: 0.696890]\n",
      "[Epoch 60/1000] [Batch 246/247] [D loss: 0.709432] [G loss: 0.695182]\n",
      "[Epoch 61/1000] [Batch 246/247] [D loss: 0.690662] [G loss: 0.689966]\n",
      "[Epoch 62/1000] [Batch 246/247] [D loss: 0.686946] [G loss: 0.686004]\n",
      "[Epoch 63/1000] [Batch 246/247] [D loss: 0.701442] [G loss: 0.692806]\n",
      "[Epoch 64/1000] [Batch 246/247] [D loss: 0.698297] [G loss: 0.684995]\n",
      "[Epoch 65/1000] [Batch 246/247] [D loss: 0.690471] [G loss: 0.697560]\n",
      "[Epoch 66/1000] [Batch 246/247] [D loss: 0.691500] [G loss: 0.700166]\n",
      "[Epoch 67/1000] [Batch 246/247] [D loss: 0.693556] [G loss: 0.691545]\n",
      "[Epoch 68/1000] [Batch 246/247] [D loss: 0.693325] [G loss: 0.692094]\n",
      "[Epoch 69/1000] [Batch 246/247] [D loss: 0.699296] [G loss: 0.701451]\n",
      "[Epoch 70/1000] [Batch 246/247] [D loss: 0.698461] [G loss: 0.694822]\n",
      "[Epoch 71/1000] [Batch 246/247] [D loss: 0.695593] [G loss: 0.706319]\n",
      "[Epoch 72/1000] [Batch 246/247] [D loss: 0.701890] [G loss: 0.665052]\n",
      "[Epoch 73/1000] [Batch 246/247] [D loss: 0.711853] [G loss: 0.637068]\n",
      "[Epoch 74/1000] [Batch 246/247] [D loss: 0.712154] [G loss: 0.675654]\n",
      "[Epoch 75/1000] [Batch 246/247] [D loss: 0.713355] [G loss: 0.684852]\n",
      "[Epoch 76/1000] [Batch 246/247] [D loss: 0.663117] [G loss: 0.745585]\n",
      "[Epoch 77/1000] [Batch 246/247] [D loss: 0.688953] [G loss: 0.678108]\n",
      "[Epoch 78/1000] [Batch 246/247] [D loss: 0.680652] [G loss: 0.691845]\n",
      "[Epoch 79/1000] [Batch 246/247] [D loss: 0.691086] [G loss: 0.696291]\n",
      "[Epoch 80/1000] [Batch 246/247] [D loss: 0.682724] [G loss: 0.678314]\n",
      "[Epoch 81/1000] [Batch 246/247] [D loss: 0.676755] [G loss: 0.711580]\n",
      "[Epoch 82/1000] [Batch 246/247] [D loss: 0.692515] [G loss: 0.712178]\n",
      "[Epoch 83/1000] [Batch 246/247] [D loss: 0.698802] [G loss: 0.679996]\n",
      "[Epoch 84/1000] [Batch 246/247] [D loss: 0.693903] [G loss: 0.695957]\n",
      "[Epoch 85/1000] [Batch 246/247] [D loss: 0.693395] [G loss: 0.681445]\n",
      "[Epoch 86/1000] [Batch 246/247] [D loss: 0.700741] [G loss: 0.680060]\n",
      "[Epoch 87/1000] [Batch 246/247] [D loss: 0.674332] [G loss: 0.698756]\n",
      "[Epoch 88/1000] [Batch 246/247] [D loss: 0.705456] [G loss: 0.731905]\n",
      "[Epoch 89/1000] [Batch 246/247] [D loss: 0.680591] [G loss: 0.677447]\n",
      "[Epoch 90/1000] [Batch 246/247] [D loss: 0.682357] [G loss: 0.698180]\n",
      "[Epoch 91/1000] [Batch 246/247] [D loss: 0.678060] [G loss: 0.701255]\n",
      "[Epoch 92/1000] [Batch 246/247] [D loss: 0.706003] [G loss: 0.686922]\n",
      "[Epoch 93/1000] [Batch 246/247] [D loss: 0.690087] [G loss: 0.671226]\n",
      "[Epoch 94/1000] [Batch 246/247] [D loss: 0.655983] [G loss: 0.694314]\n",
      "[Epoch 95/1000] [Batch 246/247] [D loss: 0.496423] [G loss: 0.806413]\n",
      "[Epoch 96/1000] [Batch 246/247] [D loss: 0.666278] [G loss: 0.757403]\n",
      "[Epoch 97/1000] [Batch 246/247] [D loss: 0.689023] [G loss: 0.715036]\n",
      "[Epoch 98/1000] [Batch 246/247] [D loss: 0.673618] [G loss: 0.626917]\n",
      "[Epoch 99/1000] [Batch 246/247] [D loss: 0.698965] [G loss: 0.665233]\n",
      "[Epoch 100/1000] [Batch 246/247] [D loss: 0.693610] [G loss: 0.672527]\n",
      "[Epoch 101/1000] [Batch 246/247] [D loss: 0.706813] [G loss: 0.667839]\n",
      "[Epoch 102/1000] [Batch 246/247] [D loss: 0.707031] [G loss: 0.700818]\n",
      "[Epoch 103/1000] [Batch 246/247] [D loss: 0.694885] [G loss: 0.705357]\n",
      "[Epoch 104/1000] [Batch 246/247] [D loss: 0.688177] [G loss: 0.690286]\n",
      "[Epoch 105/1000] [Batch 246/247] [D loss: 0.699667] [G loss: 0.680373]\n",
      "[Epoch 106/1000] [Batch 246/247] [D loss: 0.698146] [G loss: 0.701016]\n",
      "[Epoch 107/1000] [Batch 246/247] [D loss: 0.692419] [G loss: 0.703120]\n",
      "[Epoch 108/1000] [Batch 246/247] [D loss: 0.698415] [G loss: 0.685570]\n",
      "[Epoch 109/1000] [Batch 246/247] [D loss: 0.695460] [G loss: 0.776699]\n",
      "[Epoch 110/1000] [Batch 246/247] [D loss: 0.696747] [G loss: 0.680516]\n",
      "[Epoch 111/1000] [Batch 246/247] [D loss: 0.721351] [G loss: 0.693725]\n",
      "[Epoch 112/1000] [Batch 246/247] [D loss: 0.671909] [G loss: 0.697783]\n",
      "[Epoch 113/1000] [Batch 246/247] [D loss: 0.699318] [G loss: 0.672795]\n",
      "[Epoch 114/1000] [Batch 246/247] [D loss: 0.687871] [G loss: 0.703695]\n",
      "[Epoch 115/1000] [Batch 246/247] [D loss: 0.712304] [G loss: 0.701836]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 116/1000] [Batch 246/247] [D loss: 0.683880] [G loss: 0.711506]\n",
      "[Epoch 117/1000] [Batch 246/247] [D loss: 0.707301] [G loss: 0.711395]\n",
      "[Epoch 118/1000] [Batch 246/247] [D loss: 0.692877] [G loss: 0.693057]\n",
      "[Epoch 119/1000] [Batch 246/247] [D loss: 0.696218] [G loss: 0.704469]\n",
      "[Epoch 120/1000] [Batch 246/247] [D loss: 0.694530] [G loss: 0.696150]\n",
      "[Epoch 121/1000] [Batch 246/247] [D loss: 0.680198] [G loss: 0.703968]\n",
      "[Epoch 122/1000] [Batch 246/247] [D loss: 0.694046] [G loss: 0.705082]\n",
      "[Epoch 123/1000] [Batch 246/247] [D loss: 0.687916] [G loss: 0.716988]\n",
      "[Epoch 124/1000] [Batch 246/247] [D loss: 0.687908] [G loss: 0.695407]\n",
      "[Epoch 125/1000] [Batch 246/247] [D loss: 0.680871] [G loss: 0.746733]\n",
      "[Epoch 126/1000] [Batch 246/247] [D loss: 0.684452] [G loss: 0.706196]\n",
      "[Epoch 127/1000] [Batch 246/247] [D loss: 0.678498] [G loss: 0.711816]\n",
      "[Epoch 128/1000] [Batch 246/247] [D loss: 0.675448] [G loss: 0.715674]\n",
      "[Epoch 129/1000] [Batch 246/247] [D loss: 0.682618] [G loss: 0.662822]\n",
      "[Epoch 130/1000] [Batch 246/247] [D loss: 0.682138] [G loss: 0.655157]\n",
      "[Epoch 131/1000] [Batch 246/247] [D loss: 0.715506] [G loss: 0.737258]\n",
      "[Epoch 132/1000] [Batch 246/247] [D loss: 0.709536] [G loss: 0.698415]\n",
      "[Epoch 133/1000] [Batch 246/247] [D loss: 0.701157] [G loss: 0.676191]\n",
      "[Epoch 134/1000] [Batch 246/247] [D loss: 0.666698] [G loss: 0.688724]\n",
      "[Epoch 135/1000] [Batch 246/247] [D loss: 0.689263] [G loss: 0.670880]\n",
      "[Epoch 136/1000] [Batch 246/247] [D loss: 0.685466] [G loss: 0.717631]\n",
      "[Epoch 137/1000] [Batch 246/247] [D loss: 0.693576] [G loss: 0.700335]\n",
      "[Epoch 138/1000] [Batch 246/247] [D loss: 0.690345] [G loss: 0.692061]\n",
      "[Epoch 139/1000] [Batch 246/247] [D loss: 0.688637] [G loss: 0.705467]\n",
      "[Epoch 140/1000] [Batch 246/247] [D loss: 0.700454] [G loss: 0.693118]\n",
      "[Epoch 141/1000] [Batch 246/247] [D loss: 0.684018] [G loss: 0.720428]\n",
      "[Epoch 142/1000] [Batch 246/247] [D loss: 0.681313] [G loss: 0.729222]\n",
      "[Epoch 143/1000] [Batch 246/247] [D loss: 0.692246] [G loss: 0.698205]\n",
      "[Epoch 144/1000] [Batch 246/247] [D loss: 0.697904] [G loss: 0.713959]\n",
      "[Epoch 145/1000] [Batch 246/247] [D loss: 0.700856] [G loss: 0.670755]\n",
      "[Epoch 146/1000] [Batch 246/247] [D loss: 0.685246] [G loss: 0.695365]\n",
      "[Epoch 147/1000] [Batch 246/247] [D loss: 0.677804] [G loss: 0.675308]\n",
      "[Epoch 148/1000] [Batch 246/247] [D loss: 0.689818] [G loss: 0.678661]\n",
      "[Epoch 149/1000] [Batch 246/247] [D loss: 0.689434] [G loss: 0.683181]\n",
      "[Epoch 150/1000] [Batch 246/247] [D loss: 0.693597] [G loss: 0.684550]\n",
      "[Epoch 151/1000] [Batch 246/247] [D loss: 0.697514] [G loss: 0.681369]\n",
      "[Epoch 152/1000] [Batch 246/247] [D loss: 0.681317] [G loss: 0.681130]\n",
      "[Epoch 153/1000] [Batch 246/247] [D loss: 0.685670] [G loss: 0.682132]\n",
      "[Epoch 154/1000] [Batch 246/247] [D loss: 0.701002] [G loss: 0.706562]\n",
      "[Epoch 155/1000] [Batch 246/247] [D loss: 0.697836] [G loss: 0.681531]\n",
      "[Epoch 156/1000] [Batch 246/247] [D loss: 0.637798] [G loss: 0.775896]\n",
      "[Epoch 157/1000] [Batch 246/247] [D loss: 0.687083] [G loss: 0.677548]\n",
      "[Epoch 158/1000] [Batch 246/247] [D loss: 0.686068] [G loss: 0.688380]\n",
      "[Epoch 159/1000] [Batch 246/247] [D loss: 0.669181] [G loss: 0.715210]\n",
      "[Epoch 160/1000] [Batch 246/247] [D loss: 0.693978] [G loss: 0.685524]\n",
      "[Epoch 161/1000] [Batch 246/247] [D loss: 0.697036] [G loss: 0.700864]\n",
      "[Epoch 162/1000] [Batch 246/247] [D loss: 0.703478] [G loss: 0.715522]\n",
      "[Epoch 163/1000] [Batch 246/247] [D loss: 0.690682] [G loss: 0.714145]\n",
      "[Epoch 164/1000] [Batch 246/247] [D loss: 0.705171] [G loss: 0.698969]\n",
      "[Epoch 165/1000] [Batch 246/247] [D loss: 0.693377] [G loss: 0.695502]\n",
      "[Epoch 166/1000] [Batch 246/247] [D loss: 0.684116] [G loss: 0.708241]\n",
      "[Epoch 167/1000] [Batch 246/247] [D loss: 0.698464] [G loss: 0.691501]\n",
      "[Epoch 168/1000] [Batch 246/247] [D loss: 0.685617] [G loss: 0.671761]\n",
      "[Epoch 169/1000] [Batch 246/247] [D loss: 0.677792] [G loss: 0.717555]\n",
      "[Epoch 170/1000] [Batch 246/247] [D loss: 0.732414] [G loss: 0.662643]\n",
      "[Epoch 171/1000] [Batch 246/247] [D loss: 0.688315] [G loss: 0.696818]\n",
      "[Epoch 172/1000] [Batch 246/247] [D loss: 0.704883] [G loss: 0.701633]\n",
      "[Epoch 173/1000] [Batch 246/247] [D loss: 0.696483] [G loss: 0.628168]\n",
      "[Epoch 174/1000] [Batch 246/247] [D loss: 0.702572] [G loss: 0.726767]\n",
      "[Epoch 175/1000] [Batch 246/247] [D loss: 0.644492] [G loss: 0.724412]\n",
      "[Epoch 176/1000] [Batch 246/247] [D loss: 0.681128] [G loss: 0.710096]\n",
      "[Epoch 177/1000] [Batch 246/247] [D loss: 0.650347] [G loss: 0.709525]\n",
      "[Epoch 178/1000] [Batch 246/247] [D loss: 0.703151] [G loss: 0.677269]\n",
      "[Epoch 179/1000] [Batch 246/247] [D loss: 0.703499] [G loss: 0.614445]\n",
      "[Epoch 180/1000] [Batch 246/247] [D loss: 0.719108] [G loss: 0.678514]\n",
      "[Epoch 181/1000] [Batch 246/247] [D loss: 0.752772] [G loss: 0.732051]\n",
      "[Epoch 182/1000] [Batch 246/247] [D loss: 0.703548] [G loss: 0.702581]\n",
      "[Epoch 183/1000] [Batch 246/247] [D loss: 0.703829] [G loss: 0.687252]\n",
      "[Epoch 184/1000] [Batch 246/247] [D loss: 0.728137] [G loss: 0.707506]\n",
      "[Epoch 185/1000] [Batch 246/247] [D loss: 0.720936] [G loss: 0.691547]\n",
      "[Epoch 186/1000] [Batch 246/247] [D loss: 0.702501] [G loss: 0.711220]\n",
      "[Epoch 187/1000] [Batch 246/247] [D loss: 0.676857] [G loss: 0.677082]\n",
      "[Epoch 188/1000] [Batch 246/247] [D loss: 0.703655] [G loss: 0.703091]\n",
      "[Epoch 189/1000] [Batch 246/247] [D loss: 0.709265] [G loss: 0.698143]\n",
      "[Epoch 190/1000] [Batch 246/247] [D loss: 0.691457] [G loss: 0.683257]\n",
      "[Epoch 191/1000] [Batch 246/247] [D loss: 0.689106] [G loss: 0.683803]\n",
      "[Epoch 192/1000] [Batch 246/247] [D loss: 0.691561] [G loss: 0.703529]\n",
      "[Epoch 193/1000] [Batch 246/247] [D loss: 0.690610] [G loss: 0.682199]\n",
      "[Epoch 194/1000] [Batch 246/247] [D loss: 0.703039] [G loss: 0.667971]\n",
      "[Epoch 195/1000] [Batch 246/247] [D loss: 0.672738] [G loss: 0.685991]\n",
      "[Epoch 196/1000] [Batch 246/247] [D loss: 0.684427] [G loss: 0.714161]\n",
      "[Epoch 197/1000] [Batch 246/247] [D loss: 0.692674] [G loss: 0.734719]\n",
      "[Epoch 198/1000] [Batch 246/247] [D loss: 0.536525] [G loss: 0.736834]\n",
      "[Epoch 199/1000] [Batch 246/247] [D loss: 0.687858] [G loss: 0.698045]\n",
      "[Epoch 200/1000] [Batch 246/247] [D loss: 0.710789] [G loss: 0.702277]\n",
      "[Epoch 201/1000] [Batch 246/247] [D loss: 0.694413] [G loss: 0.699406]\n",
      "[Epoch 202/1000] [Batch 246/247] [D loss: 0.691256] [G loss: 0.705954]\n",
      "[Epoch 203/1000] [Batch 246/247] [D loss: 0.706766] [G loss: 0.718573]\n",
      "[Epoch 204/1000] [Batch 246/247] [D loss: 0.693304] [G loss: 0.669583]\n",
      "[Epoch 205/1000] [Batch 246/247] [D loss: 0.687166] [G loss: 0.694942]\n",
      "[Epoch 206/1000] [Batch 246/247] [D loss: 0.673386] [G loss: 0.697353]\n",
      "[Epoch 207/1000] [Batch 246/247] [D loss: 0.674490] [G loss: 0.691699]\n",
      "[Epoch 208/1000] [Batch 246/247] [D loss: 0.710975] [G loss: 0.693814]\n",
      "[Epoch 209/1000] [Batch 246/247] [D loss: 0.683163] [G loss: 0.710080]\n",
      "[Epoch 210/1000] [Batch 246/247] [D loss: 0.694435] [G loss: 0.672323]\n",
      "[Epoch 211/1000] [Batch 246/247] [D loss: 0.717777] [G loss: 0.726877]\n",
      "[Epoch 212/1000] [Batch 246/247] [D loss: 0.690855] [G loss: 0.683260]\n",
      "[Epoch 213/1000] [Batch 246/247] [D loss: 0.669991] [G loss: 0.716360]\n",
      "[Epoch 214/1000] [Batch 246/247] [D loss: 0.700620] [G loss: 0.684214]\n",
      "[Epoch 215/1000] [Batch 246/247] [D loss: 0.689801] [G loss: 0.688879]\n",
      "[Epoch 216/1000] [Batch 246/247] [D loss: 0.694526] [G loss: 0.710246]\n",
      "[Epoch 217/1000] [Batch 246/247] [D loss: 0.694706] [G loss: 0.679584]\n",
      "[Epoch 218/1000] [Batch 246/247] [D loss: 0.672735] [G loss: 0.671958]\n",
      "[Epoch 219/1000] [Batch 246/247] [D loss: 0.694388] [G loss: 0.671947]\n",
      "[Epoch 220/1000] [Batch 246/247] [D loss: 0.696478] [G loss: 0.681109]\n",
      "[Epoch 221/1000] [Batch 246/247] [D loss: 0.657307] [G loss: 0.675199]\n",
      "[Epoch 222/1000] [Batch 246/247] [D loss: 0.676628] [G loss: 0.664055]\n",
      "[Epoch 223/1000] [Batch 246/247] [D loss: 0.704487] [G loss: 0.736817]\n",
      "[Epoch 224/1000] [Batch 246/247] [D loss: 0.676224] [G loss: 0.704772]\n",
      "[Epoch 225/1000] [Batch 246/247] [D loss: 0.673607] [G loss: 0.697855]\n",
      "[Epoch 226/1000] [Batch 246/247] [D loss: 0.700700] [G loss: 0.693555]\n",
      "[Epoch 227/1000] [Batch 246/247] [D loss: 0.704489] [G loss: 0.672079]\n",
      "[Epoch 228/1000] [Batch 246/247] [D loss: 0.677660] [G loss: 0.722043]\n",
      "[Epoch 229/1000] [Batch 246/247] [D loss: 0.663047] [G loss: 0.752992]\n",
      "[Epoch 230/1000] [Batch 246/247] [D loss: 0.758743] [G loss: 0.648506]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 231/1000] [Batch 246/247] [D loss: 0.748431] [G loss: 0.676746]\n",
      "[Epoch 232/1000] [Batch 246/247] [D loss: 0.718265] [G loss: 0.708099]\n",
      "[Epoch 233/1000] [Batch 246/247] [D loss: 0.699739] [G loss: 0.702914]\n",
      "[Epoch 234/1000] [Batch 246/247] [D loss: 0.664141] [G loss: 0.723364]\n",
      "[Epoch 235/1000] [Batch 246/247] [D loss: 0.660798] [G loss: 0.685996]\n",
      "[Epoch 236/1000] [Batch 246/247] [D loss: 0.697227] [G loss: 0.681434]\n",
      "[Epoch 237/1000] [Batch 246/247] [D loss: 0.671368] [G loss: 0.708908]\n",
      "[Epoch 238/1000] [Batch 246/247] [D loss: 0.718141] [G loss: 0.685548]\n",
      "[Epoch 239/1000] [Batch 246/247] [D loss: 0.702648] [G loss: 0.693329]\n",
      "[Epoch 240/1000] [Batch 246/247] [D loss: 0.684115] [G loss: 0.703644]\n",
      "[Epoch 241/1000] [Batch 246/247] [D loss: 0.707892] [G loss: 0.678019]\n",
      "[Epoch 242/1000] [Batch 246/247] [D loss: 0.687991] [G loss: 0.708586]\n",
      "[Epoch 243/1000] [Batch 246/247] [D loss: 0.699021] [G loss: 0.715744]\n",
      "[Epoch 244/1000] [Batch 246/247] [D loss: 0.680951] [G loss: 0.707384]\n",
      "[Epoch 245/1000] [Batch 246/247] [D loss: 0.689005] [G loss: 0.690952]\n",
      "[Epoch 246/1000] [Batch 246/247] [D loss: 0.692726] [G loss: 0.683609]\n",
      "[Epoch 247/1000] [Batch 246/247] [D loss: 0.698007] [G loss: 0.703613]\n",
      "[Epoch 248/1000] [Batch 246/247] [D loss: 0.703565] [G loss: 0.697961]\n",
      "[Epoch 249/1000] [Batch 246/247] [D loss: 0.692436] [G loss: 0.705939]\n",
      "[Epoch 250/1000] [Batch 246/247] [D loss: 0.700498] [G loss: 0.701377]\n",
      "[Epoch 251/1000] [Batch 246/247] [D loss: 0.692994] [G loss: 0.677150]\n",
      "[Epoch 252/1000] [Batch 246/247] [D loss: 0.694353] [G loss: 0.706066]\n",
      "[Epoch 253/1000] [Batch 246/247] [D loss: 0.689433] [G loss: 0.687753]\n",
      "[Epoch 254/1000] [Batch 246/247] [D loss: 0.696889] [G loss: 0.706012]\n",
      "[Epoch 255/1000] [Batch 246/247] [D loss: 0.691724] [G loss: 0.671630]\n",
      "[Epoch 256/1000] [Batch 246/247] [D loss: 0.699468] [G loss: 0.738451]\n",
      "[Epoch 257/1000] [Batch 246/247] [D loss: 0.678390] [G loss: 0.724689]\n",
      "[Epoch 258/1000] [Batch 246/247] [D loss: 0.690063] [G loss: 0.707859]\n",
      "[Epoch 259/1000] [Batch 246/247] [D loss: 0.684958] [G loss: 0.689663]\n",
      "[Epoch 260/1000] [Batch 246/247] [D loss: 0.675266] [G loss: 0.691027]\n",
      "[Epoch 261/1000] [Batch 246/247] [D loss: 0.678330] [G loss: 0.721524]\n",
      "[Epoch 262/1000] [Batch 246/247] [D loss: 0.702222] [G loss: 0.679151]\n",
      "[Epoch 263/1000] [Batch 246/247] [D loss: 0.732358] [G loss: 0.662290]\n",
      "[Epoch 264/1000] [Batch 246/247] [D loss: 0.706245] [G loss: 0.677786]\n",
      "[Epoch 265/1000] [Batch 246/247] [D loss: 0.684962] [G loss: 0.657188]\n",
      "[Epoch 266/1000] [Batch 246/247] [D loss: 0.652564] [G loss: 0.741879]\n",
      "[Epoch 267/1000] [Batch 246/247] [D loss: 0.698142] [G loss: 0.705589]\n",
      "[Epoch 268/1000] [Batch 246/247] [D loss: 0.715666] [G loss: 0.660218]\n",
      "[Epoch 269/1000] [Batch 246/247] [D loss: 0.735223] [G loss: 0.605782]\n",
      "[Epoch 270/1000] [Batch 246/247] [D loss: 0.654187] [G loss: 0.757673]\n",
      "[Epoch 271/1000] [Batch 246/247] [D loss: 0.710754] [G loss: 0.765992]\n",
      "[Epoch 272/1000] [Batch 246/247] [D loss: 0.669852] [G loss: 0.795392]\n",
      "[Epoch 273/1000] [Batch 246/247] [D loss: 0.710227] [G loss: 0.735073]\n",
      "[Epoch 274/1000] [Batch 246/247] [D loss: 0.698476] [G loss: 0.643059]\n",
      "[Epoch 275/1000] [Batch 246/247] [D loss: 0.679376] [G loss: 0.667140]\n",
      "[Epoch 276/1000] [Batch 246/247] [D loss: 0.658303] [G loss: 0.703968]\n",
      "[Epoch 277/1000] [Batch 246/247] [D loss: 0.675922] [G loss: 0.746523]\n",
      "[Epoch 278/1000] [Batch 246/247] [D loss: 0.691115] [G loss: 0.740288]\n",
      "[Epoch 279/1000] [Batch 246/247] [D loss: 0.690528] [G loss: 0.711930]\n",
      "[Epoch 280/1000] [Batch 246/247] [D loss: 0.661370] [G loss: 0.711057]\n",
      "[Epoch 281/1000] [Batch 246/247] [D loss: 0.696294] [G loss: 0.739128]\n",
      "[Epoch 282/1000] [Batch 246/247] [D loss: 0.714712] [G loss: 0.659551]\n",
      "[Epoch 283/1000] [Batch 246/247] [D loss: 0.681165] [G loss: 0.671420]\n",
      "[Epoch 284/1000] [Batch 246/247] [D loss: 0.682272] [G loss: 0.727054]\n",
      "[Epoch 285/1000] [Batch 246/247] [D loss: 0.674428] [G loss: 0.692084]\n",
      "[Epoch 286/1000] [Batch 246/247] [D loss: 0.674650] [G loss: 0.706834]\n",
      "[Epoch 287/1000] [Batch 246/247] [D loss: 0.683487] [G loss: 0.639492]\n",
      "[Epoch 288/1000] [Batch 246/247] [D loss: 0.697361] [G loss: 0.685749]\n",
      "[Epoch 289/1000] [Batch 246/247] [D loss: 0.671730] [G loss: 0.682410]\n",
      "[Epoch 290/1000] [Batch 246/247] [D loss: 0.658702] [G loss: 0.773187]\n",
      "[Epoch 291/1000] [Batch 246/247] [D loss: 0.694850] [G loss: 0.702210]\n",
      "[Epoch 292/1000] [Batch 246/247] [D loss: 0.653274] [G loss: 0.758548]\n",
      "[Epoch 293/1000] [Batch 246/247] [D loss: 0.822329] [G loss: 0.664404]\n",
      "[Epoch 294/1000] [Batch 246/247] [D loss: 0.688587] [G loss: 0.681871]\n",
      "[Epoch 295/1000] [Batch 246/247] [D loss: 0.656632] [G loss: 0.660958]\n",
      "[Epoch 296/1000] [Batch 246/247] [D loss: 0.692403] [G loss: 0.707655]\n",
      "[Epoch 297/1000] [Batch 246/247] [D loss: 0.692756] [G loss: 0.663153]\n",
      "[Epoch 298/1000] [Batch 246/247] [D loss: 0.688147] [G loss: 0.655312]\n",
      "[Epoch 299/1000] [Batch 246/247] [D loss: 0.694897] [G loss: 0.623838]\n",
      "[Epoch 300/1000] [Batch 246/247] [D loss: 0.700069] [G loss: 0.731040]\n",
      "[Epoch 301/1000] [Batch 246/247] [D loss: 0.716829] [G loss: 0.758670]\n",
      "[Epoch 302/1000] [Batch 246/247] [D loss: 0.755367] [G loss: 0.768566]\n",
      "[Epoch 303/1000] [Batch 246/247] [D loss: 0.597210] [G loss: 0.691999]\n",
      "[Epoch 304/1000] [Batch 246/247] [D loss: 0.651365] [G loss: 0.733216]\n",
      "[Epoch 305/1000] [Batch 246/247] [D loss: 0.702508] [G loss: 0.694350]\n",
      "[Epoch 306/1000] [Batch 246/247] [D loss: 0.688083] [G loss: 0.660081]\n",
      "[Epoch 307/1000] [Batch 246/247] [D loss: 0.649734] [G loss: 0.683320]\n",
      "[Epoch 308/1000] [Batch 246/247] [D loss: 0.689705] [G loss: 0.750579]\n",
      "[Epoch 309/1000] [Batch 246/247] [D loss: 0.763258] [G loss: 0.716935]\n",
      "[Epoch 310/1000] [Batch 246/247] [D loss: 0.660453] [G loss: 0.663944]\n",
      "[Epoch 311/1000] [Batch 246/247] [D loss: 0.683734] [G loss: 0.695205]\n",
      "[Epoch 312/1000] [Batch 246/247] [D loss: 0.692890] [G loss: 0.621672]\n",
      "[Epoch 313/1000] [Batch 246/247] [D loss: 0.700978] [G loss: 0.774775]\n",
      "[Epoch 314/1000] [Batch 246/247] [D loss: 0.717530] [G loss: 0.704780]\n",
      "[Epoch 315/1000] [Batch 246/247] [D loss: 0.699158] [G loss: 0.752968]\n",
      "[Epoch 316/1000] [Batch 246/247] [D loss: 0.692483] [G loss: 0.649015]\n",
      "[Epoch 317/1000] [Batch 246/247] [D loss: 0.693200] [G loss: 0.701712]\n",
      "[Epoch 318/1000] [Batch 246/247] [D loss: 0.680886] [G loss: 0.711163]\n",
      "[Epoch 319/1000] [Batch 246/247] [D loss: 0.683312] [G loss: 0.628419]\n",
      "[Epoch 320/1000] [Batch 246/247] [D loss: 0.712601] [G loss: 0.661732]\n",
      "[Epoch 321/1000] [Batch 246/247] [D loss: 0.720986] [G loss: 0.675854]\n",
      "[Epoch 322/1000] [Batch 246/247] [D loss: 0.714385] [G loss: 0.676123]\n",
      "[Epoch 323/1000] [Batch 246/247] [D loss: 0.706873] [G loss: 0.679223]\n",
      "[Epoch 324/1000] [Batch 246/247] [D loss: 0.637707] [G loss: 0.811453]\n",
      "[Epoch 325/1000] [Batch 246/247] [D loss: 0.709576] [G loss: 0.768340]\n",
      "[Epoch 326/1000] [Batch 246/247] [D loss: 0.640277] [G loss: 0.689337]\n",
      "[Epoch 327/1000] [Batch 246/247] [D loss: 0.712672] [G loss: 0.693018]\n",
      "[Epoch 328/1000] [Batch 246/247] [D loss: 0.633152] [G loss: 0.810948]\n",
      "[Epoch 329/1000] [Batch 246/247] [D loss: 0.669674] [G loss: 0.574419]\n",
      "[Epoch 330/1000] [Batch 246/247] [D loss: 0.650374] [G loss: 0.778692]\n",
      "[Epoch 331/1000] [Batch 246/247] [D loss: 0.658569] [G loss: 0.657328]\n",
      "[Epoch 332/1000] [Batch 246/247] [D loss: 0.768119] [G loss: 0.701478]\n",
      "[Epoch 333/1000] [Batch 246/247] [D loss: 0.763702] [G loss: 0.726005]\n",
      "[Epoch 334/1000] [Batch 246/247] [D loss: 0.722507] [G loss: 0.637047]\n",
      "[Epoch 335/1000] [Batch 246/247] [D loss: 0.644601] [G loss: 0.803550]\n",
      "[Epoch 336/1000] [Batch 246/247] [D loss: 0.587503] [G loss: 0.780967]\n",
      "[Epoch 337/1000] [Batch 246/247] [D loss: 0.689542] [G loss: 0.657768]\n",
      "[Epoch 338/1000] [Batch 246/247] [D loss: 0.621203] [G loss: 0.916699]\n",
      "[Epoch 339/1000] [Batch 246/247] [D loss: 0.677140] [G loss: 0.532078]\n",
      "[Epoch 340/1000] [Batch 246/247] [D loss: 0.762770] [G loss: 0.675221]\n",
      "[Epoch 341/1000] [Batch 246/247] [D loss: 0.657064] [G loss: 0.679889]\n",
      "[Epoch 342/1000] [Batch 246/247] [D loss: 0.610825] [G loss: 0.660533]\n",
      "[Epoch 343/1000] [Batch 246/247] [D loss: 0.513621] [G loss: 0.658483]\n",
      "[Epoch 344/1000] [Batch 246/247] [D loss: 0.618154] [G loss: 0.654778]\n",
      "[Epoch 345/1000] [Batch 246/247] [D loss: 0.685929] [G loss: 0.780212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 346/1000] [Batch 246/247] [D loss: 0.714936] [G loss: 0.544403]\n",
      "[Epoch 347/1000] [Batch 246/247] [D loss: 0.503740] [G loss: 0.833417]\n",
      "[Epoch 348/1000] [Batch 246/247] [D loss: 0.776627] [G loss: 0.587471]\n",
      "[Epoch 349/1000] [Batch 246/247] [D loss: 0.892745] [G loss: 0.849224]\n",
      "[Epoch 350/1000] [Batch 246/247] [D loss: 0.668705] [G loss: 0.873441]\n",
      "[Epoch 351/1000] [Batch 246/247] [D loss: 0.582360] [G loss: 0.725337]\n",
      "[Epoch 352/1000] [Batch 246/247] [D loss: 0.575313] [G loss: 0.800100]\n",
      "[Epoch 353/1000] [Batch 246/247] [D loss: 0.930144] [G loss: 0.906160]\n",
      "[Epoch 354/1000] [Batch 246/247] [D loss: 0.810024] [G loss: 0.753771]\n",
      "[Epoch 355/1000] [Batch 246/247] [D loss: 1.236760] [G loss: 1.330255]\n",
      "[Epoch 356/1000] [Batch 246/247] [D loss: 0.635470] [G loss: 1.160454]\n",
      "[Epoch 357/1000] [Batch 246/247] [D loss: 0.598530] [G loss: 0.593256]\n",
      "[Epoch 358/1000] [Batch 246/247] [D loss: 0.753454] [G loss: 0.580669]\n",
      "[Epoch 359/1000] [Batch 246/247] [D loss: 0.994078] [G loss: 0.728437]\n",
      "[Epoch 360/1000] [Batch 246/247] [D loss: 0.884590] [G loss: 1.602347]\n",
      "[Epoch 361/1000] [Batch 246/247] [D loss: 0.788826] [G loss: 1.164514]\n",
      "[Epoch 362/1000] [Batch 246/247] [D loss: 1.185389] [G loss: 1.407320]\n",
      "[Epoch 363/1000] [Batch 246/247] [D loss: 0.565302] [G loss: 2.008972]\n",
      "[Epoch 364/1000] [Batch 246/247] [D loss: 0.374507] [G loss: 0.493668]\n",
      "[Epoch 365/1000] [Batch 246/247] [D loss: 0.551886] [G loss: 0.606391]\n",
      "[Epoch 366/1000] [Batch 246/247] [D loss: 0.298398] [G loss: 0.589634]\n",
      "[Epoch 367/1000] [Batch 246/247] [D loss: 0.571071] [G loss: 1.089016]\n",
      "[Epoch 368/1000] [Batch 246/247] [D loss: 0.733745] [G loss: 0.620048]\n",
      "[Epoch 369/1000] [Batch 246/247] [D loss: 0.709917] [G loss: 1.633974]\n",
      "[Epoch 370/1000] [Batch 246/247] [D loss: 0.590088] [G loss: 2.423373]\n",
      "[Epoch 371/1000] [Batch 246/247] [D loss: 0.963124] [G loss: 0.535494]\n",
      "[Epoch 372/1000] [Batch 246/247] [D loss: 0.628694] [G loss: 0.820444]\n",
      "[Epoch 373/1000] [Batch 246/247] [D loss: 0.149879] [G loss: 0.788576]\n",
      "[Epoch 374/1000] [Batch 246/247] [D loss: 1.623241] [G loss: 1.019411]\n",
      "[Epoch 375/1000] [Batch 246/247] [D loss: 0.587075] [G loss: 0.712732]\n",
      "[Epoch 376/1000] [Batch 246/247] [D loss: 0.838269] [G loss: 0.584153]\n",
      "[Epoch 377/1000] [Batch 246/247] [D loss: 1.428888] [G loss: 0.416176]\n",
      "[Epoch 378/1000] [Batch 246/247] [D loss: 0.412763] [G loss: 1.214119]\n",
      "[Epoch 379/1000] [Batch 246/247] [D loss: 0.617908] [G loss: 1.029145]\n",
      "[Epoch 380/1000] [Batch 246/247] [D loss: 0.771470] [G loss: 0.997194]\n",
      "[Epoch 381/1000] [Batch 246/247] [D loss: 0.435344] [G loss: 1.373628]\n",
      "[Epoch 382/1000] [Batch 246/247] [D loss: 0.312793] [G loss: 0.844656]\n",
      "[Epoch 383/1000] [Batch 246/247] [D loss: 0.247209] [G loss: 0.374400]\n",
      "[Epoch 384/1000] [Batch 246/247] [D loss: 1.253389] [G loss: 0.307090]\n",
      "[Epoch 385/1000] [Batch 246/247] [D loss: 0.795618] [G loss: 0.624413]\n",
      "[Epoch 386/1000] [Batch 246/247] [D loss: 0.500274] [G loss: 1.088613]\n",
      "[Epoch 387/1000] [Batch 246/247] [D loss: 1.109755] [G loss: 0.836000]\n",
      "[Epoch 388/1000] [Batch 246/247] [D loss: 1.426090] [G loss: 0.735263]\n",
      "[Epoch 389/1000] [Batch 246/247] [D loss: 0.548444] [G loss: 0.442764]\n",
      "[Epoch 390/1000] [Batch 246/247] [D loss: 1.119625] [G loss: 2.344682]\n",
      "[Epoch 391/1000] [Batch 246/247] [D loss: 0.827943] [G loss: 1.470864]\n",
      "[Epoch 392/1000] [Batch 246/247] [D loss: 0.692809] [G loss: 0.679354]\n",
      "[Epoch 393/1000] [Batch 246/247] [D loss: 0.427595] [G loss: 1.619061]\n",
      "[Epoch 394/1000] [Batch 246/247] [D loss: 0.927263] [G loss: 1.164794]\n",
      "[Epoch 395/1000] [Batch 246/247] [D loss: 0.664138] [G loss: 0.656421]\n",
      "[Epoch 396/1000] [Batch 246/247] [D loss: 0.859539] [G loss: 0.441032]\n",
      "[Epoch 397/1000] [Batch 246/247] [D loss: 0.428431] [G loss: 0.496225]\n",
      "[Epoch 398/1000] [Batch 246/247] [D loss: 0.383521] [G loss: 1.284810]\n",
      "[Epoch 399/1000] [Batch 246/247] [D loss: 0.644230] [G loss: 1.164691]\n",
      "[Epoch 400/1000] [Batch 246/247] [D loss: 0.590089] [G loss: 0.635641]\n",
      "[Epoch 401/1000] [Batch 246/247] [D loss: 0.794244] [G loss: 1.141610]\n",
      "[Epoch 402/1000] [Batch 246/247] [D loss: 0.671830] [G loss: 1.090347]\n",
      "[Epoch 403/1000] [Batch 246/247] [D loss: 0.535626] [G loss: 1.624903]\n",
      "[Epoch 404/1000] [Batch 246/247] [D loss: 1.170236] [G loss: 0.773541]\n",
      "[Epoch 405/1000] [Batch 246/247] [D loss: 0.366283] [G loss: 0.643711]\n",
      "[Epoch 406/1000] [Batch 246/247] [D loss: 0.932524] [G loss: 0.923732]\n",
      "[Epoch 407/1000] [Batch 246/247] [D loss: 0.520304] [G loss: 1.819229]\n",
      "[Epoch 408/1000] [Batch 21/247] [D loss: 0.679606] [G loss: 0.904686]\r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print()\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(imgs.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(Tensor(imgs.shape[0], 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(Tensor))\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        print(\n",
    "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "            % (epoch, n_epochs, i, len(dataloader), d_loss.item(), g_loss.item()),\n",
    "            end = '\\r'\n",
    "        )\n",
    "\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        if batches_done % sample_interval == 0:\n",
    "            torch.save(generator.state_dict(), 'generator.pt')\n",
    "            torch.save(discriminator.state_dict(), 'discriminator.pt')\n",
    "            save_image(gen_imgs.data[:25], \"images/%d.png\" % batches_done, nrow=5, normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e79a651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65a3ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dd7b59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0de61f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd046f70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
